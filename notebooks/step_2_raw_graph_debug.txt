"{'input_string': '<bos>What is the capital of the state containing Dallas? Let me solve this step by step.\\nStep 1:\\n\\nThe state containing Dallas is Texas.\\n\\nStep 2:\\n\\nThe capital of Texas is Austin.', 'adjacency_matrix': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [-0.0327, -0.0003,  0.0031,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0297,  0.0159,  0.0016,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0466,  0.0012, -0.0047,  ...,  0.0000,  0.0000,  0.0000]]), 'cfg': HookedTransformerConfig:\n{'NTK_by_parts_factor': 8.0,\n 'NTK_by_parts_high_freq_factor': 4.0,\n 'NTK_by_parts_low_freq_factor': 1.0,\n 'NTK_original_ctx_len': 8192,\n 'act_fn': 'gelu_pytorch_tanh',\n 'attention_dir': 'causal',\n 'attn_only': False,\n 'attn_scale': 16.0,\n 'attn_scores_soft_cap': 50.0,\n 'attn_types': ['global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local'],\n 'checkpoint_index': None,\n 'checkpoint_label_type': None,\n 'checkpoint_value': None,\n 'd_head': 256,\n 'd_mlp': 9216,\n 'd_model': 2304,\n 'd_vocab': 256000,\n 'd_vocab_out': 256000,\n 'decoder_start_token_id': None,\n 'default_prepend_bos': True,\n 'device': device(type='cuda'),\n 'dtype': torch.bfloat16,\n 'eps': 1e-06,\n 'experts_per_token': None,\n 'final_rms': True,\n 'from_checkpoint': False,\n 'gated_mlp': True,\n 'init_mode': 'gpt2',\n 'init_weights': False,\n 'initializer_range': 0.02,\n 'load_in_4bit': False,\n 'model_name': 'gemma-2-2b',\n 'n_ctx': 8192,\n 'n_devices': 1,\n 'n_heads': 8,\n 'n_key_value_heads': 4,\n 'n_layers': 26,\n 'n_params': 2146959360,\n 'normalization_type': 'RMS',\n 'num_experts': None,\n 'original_architecture': 'Gemma2ForCausalLM',\n 'output_logits_soft_cap': 30.0,\n 'parallel_attn_mlp': False,\n 'positional_embedding_type': 'rotary',\n 'post_embedding_ln': False,\n 'relative_attention_max_distance': None,\n 'relative_attention_num_buckets': None,\n 'rotary_adjacent_pairs': False,\n 'rotary_base': 10000.0,\n 'rotary_dim': 256,\n 'scale_attn_by_inverse_layer_idx': False,\n 'seed': None,\n 'tie_word_embeddings': False,\n 'tokenizer_name': 'google/gemma-2-2b',\n 'tokenizer_prepends_bos': True,\n 'trust_remote_code': False,\n 'ungroup_grouped_query_attention': False,\n 'use_NTK_by_parts_rope': False,\n 'use_attn_in': False,\n 'use_attn_result': False,\n 'use_attn_scale': True,\n 'use_hook_mlp_in': False,\n 'use_hook_tokens': False,\n 'use_local_attn': True,\n 'use_normalization_before_and_after': True,\n 'use_qk_norm': False,\n 'use_split_qkv_input': False,\n 'window_size': 4096}, 'n_pos': 45, 'active_features': tensor([[    0,     1,    96],\n        [    0,     1,   253],\n        [    0,     1,   355],\n        ...,\n        [   25,    44, 15166],\n        [   25,    44, 15350],\n        [   25,    44, 15549]], device='cuda:0'), 'logit_tokens': tensor([109, 108, 110,   1], device='cuda:0'), 'logit_probabilities': tensor([0.8750, 0.0559, 0.0125, 0.0085], device='cuda:0', dtype=torch.bfloat16), 'input_tokens': tensor([     2,   1841,    603,    573,   6037,    576,    573,   2329,  10751,\n         26865, 235336,   4371,    682,  11560,    736,   4065,    731,   4065,\n        235265,    108,   5856, 235248, 235274, 235292,    109,    651,   2329,\n         10751,  26865,    603,   9447, 235265,    109,   5856, 235248, 235284,\n        235292,    109,    651,   6037,    576,   9447,    603,  22605, 235265],\n       device='cuda:0'), 'scan': 'mntss/gemma-scope-transcoders', 'selected_features': tensor([    0,     1,     2,  ..., 40256, 40257, 40258]), 'activation_values': tensor([ 3.0781,  1.5469,  0.5078,  ...,  8.9375,  8.1250, 70.0000],\n       device='cuda:0', dtype=torch.bfloat16)}"