"{'input_string': '<bos>What is the capital of the state containing Dallas? Austin', 'adjacency_matrix': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.0039,  0.0035, -0.0008,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0361, -0.0018,  0.0030,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0315, -0.0284, -0.0035,  ...,  0.0000,  0.0000,  0.0000]]), 'cfg': HookedTransformerConfig:\n{'NTK_by_parts_factor': 8.0,\n 'NTK_by_parts_high_freq_factor': 4.0,\n 'NTK_by_parts_low_freq_factor': 1.0,\n 'NTK_original_ctx_len': 8192,\n 'act_fn': 'gelu_pytorch_tanh',\n 'attention_dir': 'causal',\n 'attn_only': False,\n 'attn_scale': 16.0,\n 'attn_scores_soft_cap': 50.0,\n 'attn_types': ['global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local',\n                'global',\n                'local'],\n 'checkpoint_index': None,\n 'checkpoint_label_type': None,\n 'checkpoint_value': None,\n 'd_head': 256,\n 'd_mlp': 9216,\n 'd_model': 2304,\n 'd_vocab': 256000,\n 'd_vocab_out': 256000,\n 'decoder_start_token_id': None,\n 'default_prepend_bos': True,\n 'device': device(type='cuda'),\n 'dtype': torch.bfloat16,\n 'eps': 1e-06,\n 'experts_per_token': None,\n 'final_rms': True,\n 'from_checkpoint': False,\n 'gated_mlp': True,\n 'init_mode': 'gpt2',\n 'init_weights': False,\n 'initializer_range': 0.02,\n 'load_in_4bit': False,\n 'model_name': 'gemma-2-2b',\n 'n_ctx': 8192,\n 'n_devices': 1,\n 'n_heads': 8,\n 'n_key_value_heads': 4,\n 'n_layers': 26,\n 'n_params': 2146959360,\n 'normalization_type': 'RMS',\n 'num_experts': None,\n 'original_architecture': 'Gemma2ForCausalLM',\n 'output_logits_soft_cap': 30.0,\n 'parallel_attn_mlp': False,\n 'positional_embedding_type': 'rotary',\n 'post_embedding_ln': False,\n 'relative_attention_max_distance': None,\n 'relative_attention_num_buckets': None,\n 'rotary_adjacent_pairs': False,\n 'rotary_base': 10000.0,\n 'rotary_dim': 256,\n 'scale_attn_by_inverse_layer_idx': False,\n 'seed': None,\n 'tie_word_embeddings': False,\n 'tokenizer_name': 'google/gemma-2-2b',\n 'tokenizer_prepends_bos': True,\n 'trust_remote_code': False,\n 'ungroup_grouped_query_attention': False,\n 'use_NTK_by_parts_rope': False,\n 'use_attn_in': False,\n 'use_attn_result': False,\n 'use_attn_scale': True,\n 'use_hook_mlp_in': False,\n 'use_hook_tokens': False,\n 'use_local_attn': True,\n 'use_normalization_before_and_after': True,\n 'use_qk_norm': False,\n 'use_split_qkv_input': False,\n 'window_size': 4096}, 'n_pos': 12, 'active_features': tensor([[    0,     1,    96],\n        [    0,     1,   253],\n        [    0,     1,   355],\n        ...,\n        [   25,    11, 14234],\n        [   25,    11, 14532],\n        [   25,    11, 15263]], device='cuda:0'), 'logit_tokens': tensor([   109, 235265, 235269, 235336,    108,  26865,    603,   2439,  22605,\n           689], device='cuda:0'), 'logit_probabilities': tensor([0.2754, 0.1885, 0.1143, 0.0540, 0.0476, 0.0420, 0.0371, 0.0289, 0.0226,\n        0.0199], device='cuda:0', dtype=torch.bfloat16), 'input_tokens': tensor([     2,   1841,    603,    573,   6037,    576,    573,   2329,  10751,\n         26865, 235336,  22605], device='cuda:0'), 'scan': 'mntss/gemma-scope-transcoders', 'selected_features': tensor([   0,    1,    2,  ..., 9710, 9711, 9712]), 'activation_values': tensor([ 3.0781,  1.5469,  0.5078,  ..., 18.7500,  7.3438, 11.4375],\n       device='cuda:0', dtype=torch.bfloat16)}"