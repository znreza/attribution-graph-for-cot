{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3bda2a-7668-4b85-a7c2-0ce7165f5e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hugging Face cache directory is now set to: /workspace/huggingface_cache\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.10.5)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [accelerate]2\u001b[0m [accelerate]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 einops-0.8.1\n",
      "Cloning repository for the first time...\n",
      "Cloning into 'repository/circuit-tracer'...\n",
      "remote: Enumerating objects: 332, done.\u001b[K\n",
      "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
      "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
      "remote: Total 332 (delta 102), reused 42 (delta 41), pack-reused 171 (from 3)\u001b[K\n",
      "Receiving objects: 100% (332/332), 2.26 MiB | 4.63 MiB/s, done.\n",
      "Resolving deltas: 100% (146/146), done.\n",
      "Processing ./repository/circuit-tracer\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (0.36.0)\n",
      "Requirement already satisfied: ipykernel<7.0.0,>=6.29.5 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (6.30.1)\n",
      "Requirement already satisfied: ipywidgets>=8.1.7 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (2.1.2)\n",
      "Collecting pydantic>=2.0.0 (from circuit-tracer==0.1.0)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: safetensors>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (0.6.2)\n",
      "Requirement already satisfied: tokenizers>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (0.22.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (2.8.0+cu128)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (4.67.1)\n",
      "Collecting transformer-lens>=v2.16.0 (from circuit-tracer==0.1.0)\n",
      "  Downloading transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: transformers>=4.50.0 in /usr/local/lib/python3.12/dist-packages (from circuit-tracer==0.1.0) (4.57.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (9.6.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (5.14.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (0.2.14)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.1.7->circuit-tracer==0.1.0) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.1.7->circuit-tracer==0.1.0) (3.0.15)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=8.0.0->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (4.5.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (0.7.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->circuit-tracer==0.1.0)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.0.0->circuit-tracer==0.1.0)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.0.0->circuit-tracer==0.1.0)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->circuit-tracer==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.11.0)\n",
      "Collecting beartype<0.15.0,>=0.14.1 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting better-abc<0.0.4,>=0.0.3 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting datasets>=2.7.1 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fancy-einsum>=0.0.3 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jaxtyping>=0.2.11 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading jaxtyping-0.3.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting numpy>=1.24.0 (from circuit-tracer==0.1.0)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pandas>=1.1.5 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting rich>=12.6.0 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sentencepiece (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typeguard<5.0,>=4.2 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wandb>=0.13.5 (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading wandb-0.22.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.28.1)\n",
      "Collecting xxhash (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.16.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2.5.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.6.0->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.50.0->circuit-tracer==0.1.0) (2025.11.3)\n",
      "Collecting click>=8.0.1 (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading sentry_sdk-2.43.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->circuit-tracer==0.1.0) (3.0.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=7.23.1->ipykernel<7.0.0,>=6.29.5->circuit-tracer==0.1.0) (0.2.3)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
      "Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading jaxtyping-0.3.3-py3-none-any.whl (55 kB)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
      "Downloading wandb-0.22.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sentry_sdk-2.43.0-py2.py3-none-any.whl (400 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Building wheels for collected packages: circuit-tracer, transformers-stream-generator\n",
      "  Building wheel for circuit-tracer (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for circuit-tracer: filename=circuit_tracer-0.1.0-py3-none-any.whl size=111594 sha256=b36c59eab2c1f2898e038e5b501e00cc89a01c7f2e096e99866c3c9f1ec72fb5\n",
      "  Stored in directory: /workspace/.cache/pip/wheels/70/5e/64/cdc84c23ee74e49814840b9ed77066eebb0aa0416bd77a4327\n",
      "  Building wheel for transformers-stream-generator (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12525 sha256=908a9d37b4e328e930bc3a51121f1902c40033e41eb641e3d7533569129adf64\n",
      "  Stored in directory: /workspace/.cache/pip/wheels/a8/58/d2/014cb67c3cc6def738c1b1635dbf4e3dab6fb63aba7070dce0\n",
      "Successfully built circuit-tracer transformers-stream-generator\n",
      "Installing collected packages: pytz, better-abc, xxhash, wadler-lindig, tzdata, typing-inspection, typeguard, smmap, sentry-sdk, sentencepiece, pydantic-core, pyarrow, protobuf, propcache, numpy, multidict, mdurl, frozenlist, fancy-einsum, dill, click, beartype, annotated-types, aiohappyeyeballs, yarl, pydantic, pandas, multiprocess, markdown-it-py, jaxtyping, gitdb, aiosignal, rich, gitpython, aiohttp, wandb, transformers-stream-generator, datasets, transformer-lens, circuit-tracer\n",
      "\u001b[2K  Attempting uninstall: numpy╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/40\u001b[0m [protobuf]k]\n",
      "\u001b[2K    Found existing installation: numpy 2.1.2━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/40\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling numpy-2.1.2:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/40\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.1.2━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/40\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/40\u001b[0m [circuit-tracer]m [datasets]]ss]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 beartype-0.14.1 better-abc-0.0.3 circuit-tracer-0.1.0 click-8.3.0 datasets-4.4.1 dill-0.4.0 fancy-einsum-0.0.3 frozenlist-1.8.0 gitdb-4.0.12 gitpython-3.1.45 jaxtyping-0.3.3 markdown-it-py-4.0.0 mdurl-0.1.2 multidict-6.7.0 multiprocess-0.70.18 numpy-1.26.4 pandas-2.3.3 propcache-0.4.1 protobuf-6.33.0 pyarrow-22.0.0 pydantic-2.12.4 pydantic-core-2.41.5 pytz-2025.2 rich-14.2.0 sentencepiece-0.2.1 sentry-sdk-2.43.0 smmap-5.0.2 transformer-lens-2.16.1 transformers-stream-generator-0.0.5 typeguard-4.4.4 typing-inspection-0.4.2 tzdata-2025.2 wadler-lindig-0.1.7 wandb-0.22.3 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Install packages\n",
    "# ========================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "cache_dir = \"/workspace/huggingface_cache\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Hugging Face cache directory is now set to: {os.environ['HF_HOME']}\")\n",
    "\n",
    "!pip install huggingface_hub transformers accelerate einops hf_transfer\n",
    "\n",
    "#  Clone the repository if it doesn't exist, or pull the latest changes if it does.\n",
    "!if [ -d \"repository/circuit-tracer\" ]; then \\\n",
    "    echo \"✅ Repository found. Pulling latest changes...\"; \\\n",
    "    (cd repository/circuit-tracer && git pull); \\\n",
    "else \\\n",
    "    echo \"Cloning repository for the first time...\"; \\\n",
    "    mkdir -p repository && git clone https://github.com/safety-research/circuit-tracer repository/circuit-tracer; \\\n",
    "fi\n",
    "\n",
    "!pip install ./repository/circuit-tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23122025-4dcf-45d1-ab08-fa0a08e4e84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9f08b55c4245179e1a7f6ca5e159b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Add the cloned repository to the Python path\n",
    "sys.path.append('repository/circuit-tracer')\n",
    "sys.path.append('repository/circuit-tracer/demos')\n",
    "\n",
    "# This will prompt you for your token\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28780e7a-562c-457d-b165-183e42e3ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sequential CoT Attribution Analysis\n",
    "Implementation for analyzing Chain-of-Thought reasoning using attribution graphs\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "from circuit_tracer import attribute, ReplacementModel\n",
    "from circuit_tracer.graph import Graph\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "from circuit_tracer.graph import prune_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d965df74-c4c9-45c6-8293-ac2a43d8e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Generate Attribution Graphs for Reasoning Steps\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ReasoningStep:\n",
    "    \"\"\"Represents a single reasoning step in a CoT sequence\"\"\"\n",
    "    step_idx: int\n",
    "    text: str\n",
    "    start_token_idx: int\n",
    "    end_token_idx: int\n",
    "    graph: Optional[Graph] = None\n",
    "\n",
    "\n",
    "class CoTAttributionGenerator:\n",
    "    \"\"\"Generate attribution graphs for each step in a CoT reasoning sequence\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ReplacementModel,\n",
    "        max_n_logits: int = 10,\n",
    "        desired_logit_prob: float = 0.95,\n",
    "        batch_size: int = 512,\n",
    "        max_feature_nodes: int = 4096,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.max_n_logits = max_n_logits\n",
    "        self.desired_logit_prob = desired_logit_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.max_feature_nodes = max_feature_nodes\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def parse_cot_steps(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        cot_completion: str,\n",
    "        step_delimiter: str = \"\\n\"\n",
    "    ) -> List[ReasoningStep]:\n",
    "        \"\"\"\n",
    "        Parse CoT completion into individual reasoning steps\n",
    "        \n",
    "        Args:\n",
    "            prompt: The initial prompt/question\n",
    "            cot_completion: The full CoT reasoning text\n",
    "            step_delimiter: How to split steps (default: newline)\n",
    "        \n",
    "        Returns:\n",
    "            List of ReasoningStep objects\n",
    "        \"\"\"\n",
    "        # Tokenize to get token indices\n",
    "        full_text = prompt + cot_completion\n",
    "        tokens = self.model.tokenizer.encode(full_text)\n",
    "        prompt_tokens = self.model.tokenizer.encode(prompt)\n",
    "        \n",
    "        # Split completion into steps\n",
    "        steps_text = cot_completion.split(step_delimiter)\n",
    "        steps_text = [s.strip() for s in steps_text if s.strip()]\n",
    "        \n",
    "        reasoning_steps = []\n",
    "        current_token_idx = len(prompt_tokens)\n",
    "        \n",
    "        for step_idx, step_text in enumerate(steps_text):\n",
    "            step_tokens = self.model.tokenizer.encode(step_text)\n",
    "            end_token_idx = current_token_idx + len(step_tokens)\n",
    "            \n",
    "            step = ReasoningStep(\n",
    "                step_idx=step_idx,\n",
    "                text=step_text,\n",
    "                start_token_idx=current_token_idx,\n",
    "                end_token_idx=end_token_idx\n",
    "            )\n",
    "            reasoning_steps.append(step)\n",
    "            current_token_idx = end_token_idx\n",
    "        \n",
    "        return reasoning_steps\n",
    "    \n",
    "    def generate_graph_for_step(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        reasoning_step: ReasoningStep,\n",
    "        previous_steps: Optional[List[ReasoningStep]] = None\n",
    "    ) -> Graph:\n",
    "        \"\"\"\n",
    "        Generate attribution graph for a single reasoning step\n",
    "        \n",
    "        Args:\n",
    "            prompt: The initial prompt\n",
    "            reasoning_step: The step to analyze\n",
    "            previous_steps: Previous steps for context\n",
    "        \n",
    "        Returns:\n",
    "            Attribution graph for this step\n",
    "        \"\"\"\n",
    "        # Build context: prompt + all previous steps + current step\n",
    "        context_text = prompt\n",
    "        if previous_steps:\n",
    "            context_text += \" \" + \" \".join([s.text for s in previous_steps])\n",
    "        context_text += \" \" + reasoning_step.text\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Generating graph for Step {reasoning_step.step_idx}\")\n",
    "            print(f\"Step text: {reasoning_step.text[:100]}...\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        # Generate attribution graph using circuit-tracer\n",
    "        graph = attribute(\n",
    "            prompt=context_text,\n",
    "            model=self.model,\n",
    "            max_n_logits=self.max_n_logits,\n",
    "            desired_logit_prob=self.desired_logit_prob,\n",
    "            batch_size=self.batch_size,\n",
    "            max_feature_nodes=self.max_feature_nodes,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def generate_sequential_graphs(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        cot_completion: str,\n",
    "        step_delimiter: str = \"\\n\"\n",
    "    ) -> List[ReasoningStep]:\n",
    "        \"\"\"\n",
    "        Generate attribution graphs for all steps in a CoT sequence\n",
    "        \n",
    "        Args:\n",
    "            prompt: The initial prompt\n",
    "            cot_completion: The full CoT reasoning\n",
    "            step_delimiter: How to split steps\n",
    "        \n",
    "        Returns:\n",
    "            List of ReasoningSteps with graphs attached\n",
    "        \"\"\"\n",
    "        # Parse steps\n",
    "        steps = self.parse_cot_steps(prompt, cot_completion, step_delimiter)\n",
    "        \n",
    "        # Generate graph for each step\n",
    "        for i, step in enumerate(steps):\n",
    "            previous_steps = steps[:i] if i > 0 else None\n",
    "            step.graph = self.generate_graph_for_step(\n",
    "                prompt=prompt,\n",
    "                reasoning_step=step,\n",
    "                previous_steps=previous_steps\n",
    "            )\n",
    "        \n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50a5904-8588-46da-809c-12bd084c6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Aggregate and Combine Attribution Graphs\n",
    "# based on https://github.com/safety-research/circuit-tracer/blob/main/circuit_tracer/graph.py\n",
    "# ============================================================================\n",
    "\n",
    "class GraphAggregator:\n",
    "    \"\"\"Combine and aggregate multiple attribution graphs\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_sequential_graphs(\n",
    "        reasoning_steps: List[ReasoningStep],\n",
    "        merge_strategy: str = \"union\"\n",
    "    ) -> Graph:\n",
    "        \"\"\"\n",
    "        Merge multiple graphs into a single combined graph\n",
    "        \n",
    "        Args:\n",
    "            reasoning_steps: Steps with graphs to merge\n",
    "            merge_strategy: 'union' (combine all) or 'intersection' (common only)\n",
    "        \n",
    "        Returns:\n",
    "            Merged graph\n",
    "        \"\"\"\n",
    "        if not reasoning_steps or not reasoning_steps[0].graph:\n",
    "            raise ValueError(\"No graphs to merge\")\n",
    "        \n",
    "        # Start with first graph\n",
    "        merged_graph = reasoning_steps[0].graph\n",
    "        \n",
    "        if merge_strategy == \"union\":\n",
    "            # Combine all nodes and edges\n",
    "            for step in reasoning_steps[1:]:\n",
    "                if step.graph:\n",
    "                    merged_graph = GraphAggregator._union_graphs(\n",
    "                        merged_graph, \n",
    "                        step.graph\n",
    "                    )\n",
    "        \n",
    "        elif merge_strategy == \"intersection\":\n",
    "            # Keep only common nodes/edges\n",
    "            for step in reasoning_steps[1:]:\n",
    "                if step.graph:\n",
    "                    merged_graph = GraphAggregator._intersect_graphs(\n",
    "                        merged_graph, \n",
    "                        step.graph\n",
    "                    )\n",
    "        \n",
    "        return merged_graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def _union_graphs(graph1: Graph, graph2: Graph) -> Graph:\n",
    "        \"\"\"Combine two graphs (union of nodes and edges)\"\"\"\n",
    "        # This is a simplified implementation\n",
    "        \n",
    "        combined_nodes = set(graph1.nodes) | set(graph2.nodes)\n",
    "        combined_edges = {}\n",
    "        \n",
    "        # Combine edges, summing weights for common edges\n",
    "        for edge, weight in graph1.edges.items():\n",
    "            combined_edges[edge] = weight\n",
    "        \n",
    "        for edge, weight in graph2.edges.items():\n",
    "            if edge in combined_edges:\n",
    "                combined_edges[edge] += weight\n",
    "            else:\n",
    "                combined_edges[edge] = weight\n",
    "        \n",
    "        # Create new graph (THIS PART NEEDS TO BE VALIDATED)\n",
    "        \n",
    "        return Graph(nodes=combined_nodes, edges=combined_edges)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _intersect_graphs(graph1: Graph, graph2: Graph) -> Graph:\n",
    "        \"\"\"Find common elements between two graphs\"\"\"\n",
    "        common_nodes = set(graph1.nodes) & set(graph2.nodes)\n",
    "        common_edges = {}\n",
    "        \n",
    "        # Keep only edges present in both\n",
    "        for edge, weight1 in graph1.edges.items():\n",
    "            if edge in graph2.edges:\n",
    "                # Average the weights\n",
    "                weight2 = graph2.edges[edge]\n",
    "                common_edges[edge] = (weight1 + weight2) / 2\n",
    "        \n",
    "        return Graph(nodes=common_nodes, edges=common_edges)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_graph_similarity(\n",
    "        graph1: Graph, \n",
    "        graph2: Graph,\n",
    "        method: str = \"jaccard\"\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity between two graphs\n",
    "        \n",
    "        Args:\n",
    "            graph1, graph2: Graphs to compare\n",
    "            method: 'jaccard' (node overlap) or 'edge_overlap'\n",
    "        \n",
    "        Returns:\n",
    "            Similarity score [0, 1]\n",
    "        \"\"\"\n",
    "        if method == \"jaccard\":\n",
    "            nodes1 = set(graph1.nodes)\n",
    "            nodes2 = set(graph2.nodes)\n",
    "            intersection = len(nodes1 & nodes2)\n",
    "            union = len(nodes1 | nodes2)\n",
    "            return intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        elif method == \"edge_overlap\":\n",
    "            edges1 = set(graph1.edges.keys())\n",
    "            edges2 = set(graph2.edges.keys())\n",
    "            intersection = len(edges1 & edges2)\n",
    "            union = len(edges1 | edges2)\n",
    "            return intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f09fcb-380f-4d58-b76c-f2221897a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Extract Features from Attribution Graphs\n",
    "# based on https://github.com/safety-research/circuit-tracer/blob/main/circuit_tracer/graph.py\n",
    "# ============================================================================\n",
    "\n",
    "class GraphFeatureExtractor:\n",
    "    \"\"\"Extract interpretable features from attribution graphs\"\"\"\n",
    "    \n",
    "    def __init__(self, model: ReplacementModel):\n",
    "        self.model = model\n",
    "        self.transcoder = model.transcoder if hasattr(model, 'transcoder') else None\n",
    "    \n",
    "    def extract_all_features(self, graph: Graph) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Extract comprehensive feature set from a graph\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all extracted features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Global graph statistics\n",
    "        features.update(self.extract_global_statistics(graph))\n",
    "        \n",
    "        # Node-level features\n",
    "        features.update(self.extract_node_features(graph))\n",
    "        \n",
    "        # Topological features\n",
    "        features.update(self.extract_topological_features(graph))\n",
    "        \n",
    "        # Transcoder-specific features\n",
    "        if self.transcoder:\n",
    "            features.update(self.extract_transcoder_features(graph))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_global_statistics(self, graph: Graph) -> Dict[str, float]:\n",
    "        \"\"\"Extract global graph-level statistics\"\"\"\n",
    "        features = {\n",
    "            'num_nodes': len(graph.nodes),\n",
    "            'num_edges': len(graph.edges),\n",
    "            'num_feature_nodes': sum(1 for n in graph.nodes if n.type == 'feature'),\n",
    "            'num_token_nodes': sum(1 for n in graph.nodes if n.type == 'token'),\n",
    "            'num_logit_nodes': sum(1 for n in graph.nodes if n.type == 'logit'),\n",
    "        }\n",
    "        \n",
    "        # Logit statistics\n",
    "        if hasattr(graph, 'logit_probs'):\n",
    "            features['top_logit_prob'] = max(graph.logit_probs.values())\n",
    "            features['logit_entropy'] = self._compute_entropy(\n",
    "                list(graph.logit_probs.values())\n",
    "            )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_node_features(self, graph: Graph) -> Dict[str, float]:\n",
    "        \"\"\"Extract node-level statistics\"\"\"\n",
    "        activations = []\n",
    "        influences = []\n",
    "        \n",
    "        for node in graph.nodes:\n",
    "            if hasattr(node, 'activation'):\n",
    "                activations.append(node.activation)\n",
    "            if hasattr(node, 'influence'):\n",
    "                influences.append(node.influence)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        if activations:\n",
    "            features['mean_activation'] = np.mean(activations)\n",
    "            features['max_activation'] = np.max(activations)\n",
    "            features['std_activation'] = np.std(activations)\n",
    "        \n",
    "        if influences:\n",
    "            features['mean_influence'] = np.mean(influences)\n",
    "            features['max_influence'] = np.max(influences)\n",
    "            features['total_influence'] = np.sum(influences)\n",
    "        \n",
    "        # Layer-wise histogram\n",
    "        layer_counts = defaultdict(int)\n",
    "        for node in graph.nodes:\n",
    "            if hasattr(node, 'layer'):\n",
    "                layer_counts[node.layer] += 1\n",
    "        \n",
    "        # Convert to feature vector\n",
    "        max_layers = 32  # Adjust based on model\n",
    "        for layer in range(max_layers):\n",
    "            features[f'layer_{layer}_count'] = layer_counts.get(layer, 0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_topological_features(self, graph: Graph) -> Dict[str, float]:\n",
    "        \"\"\"Extract graph topology features\"\"\"\n",
    "        # Convert to NetworkX for analysis\n",
    "        G = self._to_networkx(graph)\n",
    "        \n",
    "        features = {\n",
    "            'graph_density': nx.density(G),\n",
    "            'num_connected_components': nx.number_weakly_connected_components(G),\n",
    "        }\n",
    "        \n",
    "        # Edge statistics\n",
    "        edge_weights = [data.get('weight', 1.0) for _, _, data in G.edges(data=True)]\n",
    "        if edge_weights:\n",
    "            features['mean_edge_weight'] = np.mean(edge_weights)\n",
    "            features['max_edge_weight'] = np.max(edge_weights)\n",
    "            features['sum_edge_weights'] = np.sum(edge_weights)\n",
    "        \n",
    "        # Centrality measures\n",
    "        try:\n",
    "            degree_centrality = nx.degree_centrality(G)\n",
    "            features['mean_degree_centrality'] = np.mean(list(degree_centrality.values()))\n",
    "            features['max_degree_centrality'] = np.max(list(degree_centrality.values()))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Path-based features\n",
    "        try:\n",
    "            # Shortest paths from input to output\n",
    "            input_nodes = [n for n in G.nodes() if 'input' in str(n)]\n",
    "            output_nodes = [n for n in G.nodes() if 'output' in str(n)]\n",
    "            \n",
    "            if input_nodes and output_nodes:\n",
    "                paths = []\n",
    "                for inp in input_nodes[:5]:  # Sample a few\n",
    "                    for out in output_nodes[:5]:\n",
    "                        try:\n",
    "                            length = nx.shortest_path_length(G, inp, out)\n",
    "                            paths.append(length)\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                if paths:\n",
    "                    features['mean_path_length'] = np.mean(paths)\n",
    "                    features['min_path_length'] = np.min(paths)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_transcoder_features(self, graph: Graph) -> Dict[str, float]:\n",
    "        \"\"\"Extract features specific to transcoder activations\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Feature sparsity\n",
    "        active_features = [n for n in graph.nodes if n.type == 'feature' and n.activation > 0]\n",
    "        features['num_active_features'] = len(active_features)\n",
    "        features['feature_sparsity'] = len(active_features) / max(len(graph.nodes), 1)\n",
    "        \n",
    "        # Feature activation patterns by layer\n",
    "        layer_activations = defaultdict(list)\n",
    "        for node in active_features:\n",
    "            if hasattr(node, 'layer'):\n",
    "                layer_activations[node.layer].append(node.activation)\n",
    "        \n",
    "        for layer, acts in layer_activations.items():\n",
    "            features[f'layer_{layer}_mean_activation'] = np.mean(acts)\n",
    "            features[f'layer_{layer}_num_active'] = len(acts)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_entropy(probs: List[float]) -> float:\n",
    "        \"\"\"Compute Shannon entropy\"\"\"\n",
    "        probs = np.array(probs)\n",
    "        probs = probs / probs.sum()  # Normalize\n",
    "        return -np.sum(probs * np.log2(probs + 1e-10))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_networkx(graph: Graph) -> nx.DiGraph:\n",
    "        \"\"\"Convert Graph to NetworkX for analysis\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for node in graph.nodes:\n",
    "            G.add_node(\n",
    "                node.id,\n",
    "                type=node.type,\n",
    "                activation=getattr(node, 'activation', 0),\n",
    "                influence=getattr(node, 'influence', 0)\n",
    "            )\n",
    "        \n",
    "        # Add edges\n",
    "        for (src, tgt), weight in graph.edges.items():\n",
    "            G.add_edge(src, tgt, weight=weight)\n",
    "        \n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ba33fb-9651-40e4-b59b-7ffda7939010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Advanced Analysis Functions\n",
    "# ============================================================================\n",
    "\n",
    "class SequentialFeatureAnalyzer:\n",
    "    \"\"\"Analyze features across multiple reasoning steps\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_extractor: GraphFeatureExtractor):\n",
    "        self.extractor = feature_extractor\n",
    "    \n",
    "    def track_feature_evolution(\n",
    "        self,\n",
    "        reasoning_steps: List[ReasoningStep]\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Track how features change across reasoning steps\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping feature names to time series\n",
    "        \"\"\"\n",
    "        feature_trajectories = defaultdict(list)\n",
    "        \n",
    "        for step in reasoning_steps:\n",
    "            if step.graph:\n",
    "                features = self.extractor.extract_all_features(step.graph)\n",
    "                for feat_name, feat_value in features.items():\n",
    "                    feature_trajectories[feat_name].append(feat_value)\n",
    "        \n",
    "        return dict(feature_trajectories)\n",
    "    \n",
    "    def identify_persistent_features(\n",
    "        self,\n",
    "        reasoning_steps: List[ReasoningStep],\n",
    "        threshold: float = 0.5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Identify features that persist across multiple steps\n",
    "        \n",
    "        Args:\n",
    "            reasoning_steps: Steps to analyze\n",
    "            threshold: Minimum proportion of steps where feature must appear\n",
    "        \n",
    "        Returns:\n",
    "            List of (feature_id, persistence_score) tuples\n",
    "        \"\"\"\n",
    "        feature_presence = defaultdict(int)\n",
    "        total_steps = len(reasoning_steps)\n",
    "        \n",
    "        for step in reasoning_steps:\n",
    "            if step.graph:\n",
    "                active_features = set(\n",
    "                    n.id for n in step.graph.nodes \n",
    "                    if n.type == 'feature' and n.activation > 0\n",
    "                )\n",
    "                for feat_id in active_features:\n",
    "                    feature_presence[feat_id] += 1\n",
    "        \n",
    "        # Calculate persistence scores\n",
    "        persistent_features = [\n",
    "            (feat_id, count / total_steps)\n",
    "            for feat_id, count in feature_presence.items()\n",
    "            if count / total_steps >= threshold\n",
    "        ]\n",
    "        \n",
    "        return sorted(persistent_features, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def detect_reasoning_transitions(\n",
    "        self,\n",
    "        reasoning_steps: List[ReasoningStep]\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Detect critical transition points in reasoning\n",
    "        \n",
    "        Returns:\n",
    "            List of step indices where significant changes occur\n",
    "        \"\"\"\n",
    "        if len(reasoning_steps) < 2:\n",
    "            return []\n",
    "        \n",
    "        transitions = []\n",
    "        \n",
    "        for i in range(1, len(reasoning_steps)):\n",
    "            prev_graph = reasoning_steps[i-1].graph\n",
    "            curr_graph = reasoning_steps[i].graph\n",
    "            \n",
    "            if prev_graph and curr_graph:\n",
    "                # Compute graph similarity\n",
    "                similarity = GraphAggregator.compute_graph_similarity(\n",
    "                    prev_graph, \n",
    "                    curr_graph,\n",
    "                    method=\"jaccard\"\n",
    "                )\n",
    "                \n",
    "                # If similarity drops significantly, it's a transition\n",
    "                if similarity < 0.3:  # Threshold\n",
    "                    transitions.append(i)\n",
    "        \n",
    "        return transitions\n",
    "    \n",
    "    def compare_cot_vs_nocot(\n",
    "        self,\n",
    "        cot_steps: List[ReasoningStep],\n",
    "        nocot_graph: Graph\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compare CoT reasoning graphs with non-CoT direct answer\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of comparison metrics\n",
    "        \"\"\"\n",
    "        # Extract features from non-CoT\n",
    "        nocot_features = self.extractor.extract_all_features(nocot_graph)\n",
    "        \n",
    "        # Extract features from each CoT step\n",
    "        cot_feature_series = []\n",
    "        for step in cot_steps:\n",
    "            if step.graph:\n",
    "                features = self.extractor.extract_all_features(step.graph)\n",
    "                cot_feature_series.append(features)\n",
    "        \n",
    "        # Compute statistics\n",
    "        comparison = {}\n",
    "        \n",
    "        # Average CoT features\n",
    "        avg_cot_features = {}\n",
    "        for feat_name in nocot_features.keys():\n",
    "            values = [f.get(feat_name, 0) for f in cot_feature_series]\n",
    "            if values:\n",
    "                avg_cot_features[feat_name] = np.mean(values)\n",
    "        \n",
    "        # Compare\n",
    "        for feat_name in nocot_features.keys():\n",
    "            nocot_val = nocot_features[feat_name]\n",
    "            cot_val = avg_cot_features.get(feat_name, 0)\n",
    "            \n",
    "            if nocot_val != 0:\n",
    "                comparison[f'{feat_name}_ratio'] = cot_val / nocot_val\n",
    "            comparison[f'{feat_name}_diff'] = cot_val - nocot_val\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def aggregate_step_features(\n",
    "        self,\n",
    "        reasoning_steps: List[ReasoningStep],\n",
    "        aggregation: str = \"mean\"\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Aggregate features across all steps\n",
    "        \n",
    "        Args:\n",
    "            reasoning_steps: Steps to aggregate\n",
    "            aggregation: 'mean', 'sum', 'max', 'min'\n",
    "        \n",
    "        Returns:\n",
    "            Aggregated feature dictionary\n",
    "        \"\"\"\n",
    "        all_features = []\n",
    "        for step in reasoning_steps:\n",
    "            if step.graph:\n",
    "                features = self.extractor.extract_all_features(step.graph)\n",
    "                all_features.append(features)\n",
    "        \n",
    "        if not all_features:\n",
    "            return {}\n",
    "        \n",
    "        aggregated = {}\n",
    "        feature_names = all_features[0].keys()\n",
    "        \n",
    "        for feat_name in feature_names:\n",
    "            values = [f.get(feat_name, 0) for f in all_features]\n",
    "            \n",
    "            if aggregation == \"mean\":\n",
    "                aggregated[feat_name] = np.mean(values)\n",
    "            elif aggregation == \"sum\":\n",
    "                aggregated[feat_name] = np.sum(values)\n",
    "            elif aggregation == \"max\":\n",
    "                aggregated[feat_name] = np.max(values)\n",
    "            elif aggregation == \"min\":\n",
    "                aggregated[feat_name] = np.min(values)\n",
    "        \n",
    "        return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be211792-c9ce-4741-aca2-b4610a5f857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Visualization and Reporting\n",
    "# ============================================================================\n",
    "\n",
    "class FeatureVisualizer:\n",
    "    \"\"\"Visualize and report on extracted features\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_feature_trajectories(\n",
    "        feature_trajectories: Dict[str, List[float]],\n",
    "        feature_names: Optional[List[str]] = None,\n",
    "        save_path: Optional[Path] = None\n",
    "    ):\n",
    "        \"\"\"Plot how features evolve across reasoning steps\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if feature_names is None:\n",
    "            # Plot first 10 features\n",
    "            feature_names = list(feature_trajectories.keys())[:10]\n",
    "        \n",
    "        fig, axes = plt.subplots(len(feature_names), 1, figsize=(12, 3*len(feature_names)))\n",
    "        if len(feature_names) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, feat_name in zip(axes, feature_names):\n",
    "            values = feature_trajectories[feat_name]\n",
    "            ax.plot(values, marker='o')\n",
    "            ax.set_title(feat_name)\n",
    "            ax.set_xlabel('Reasoning Step')\n",
    "            ax.set_ylabel('Feature Value')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_feature_report(\n",
    "        reasoning_steps: List[ReasoningStep],\n",
    "        analyzer: SequentialFeatureAnalyzer\n",
    "    ) -> str:\n",
    "        \"\"\"Generate a text report of feature analysis\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\"*60)\n",
    "        report.append(\"CHAIN-OF-THOUGHT FEATURE ANALYSIS REPORT\")\n",
    "        report.append(\"=\"*60)\n",
    "        report.append(f\"\\nTotal Reasoning Steps: {len(reasoning_steps)}\")\n",
    "        \n",
    "        # Persistent features\n",
    "        persistent = analyzer.identify_persistent_features(reasoning_steps)\n",
    "        report.append(f\"\\n--- Persistent Features (appear in >50% of steps) ---\")\n",
    "        for feat_id, score in persistent[:10]:\n",
    "            report.append(f\"  {feat_id}: {score:.2%}\")\n",
    "        \n",
    "        # Transitions\n",
    "        transitions = analyzer.detect_reasoning_transitions(reasoning_steps)\n",
    "        report.append(f\"\\n--- Reasoning Transitions ---\")\n",
    "        report.append(f\"Detected {len(transitions)} significant transitions at steps: {transitions}\")\n",
    "        \n",
    "        # Feature evolution\n",
    "        trajectories = analyzer.track_feature_evolution(reasoning_steps)\n",
    "        report.append(f\"\\n--- Key Feature Trends ---\")\n",
    "        for feat_name in ['num_active_features', 'mean_influence', 'graph_density']:\n",
    "            if feat_name in trajectories:\n",
    "                values = trajectories[feat_name]\n",
    "                report.append(f\"  {feat_name}:\")\n",
    "                report.append(f\"    Start: {values[0]:.4f}, End: {values[-1]:.4f}, Change: {values[-1] - values[0]:.4f}\")\n",
    "        \n",
    "        report.append(\"\\n\" + \"=\"*60)\n",
    "        return \"\\n\".join(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781d6f3e-93f7-4ea4-9566-90419738e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: End-to-End Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "class CoTMechanisticAnalyzer:\n",
    "    \"\"\"Complete pipeline for analyzing CoT reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self, model: ReplacementModel):\n",
    "        self.model = model\n",
    "        self.graph_generator = CoTAttributionGenerator(model)\n",
    "        self.feature_extractor = GraphFeatureExtractor(model)\n",
    "        self.analyzer = SequentialFeatureAnalyzer(self.feature_extractor)\n",
    "        self.visualizer = FeatureVisualizer()\n",
    "    \n",
    "    def analyze_cot_sequence(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        cot_completion: str,\n",
    "        step_delimiter: str = \"\\n\",\n",
    "        generate_report: bool = True,\n",
    "        save_visualizations: bool = False\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete end-to-end analysis of a CoT sequence\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all analysis results\n",
    "        \"\"\"\n",
    "        print(\"Starting CoT Mechanistic Analysis...\")\n",
    "        \n",
    "        # Step 1: Generate graphs\n",
    "        print(\"\\n[1/5] Generating attribution graphs...\")\n",
    "        reasoning_steps = self.graph_generator.generate_sequential_graphs(\n",
    "            prompt=prompt,\n",
    "            cot_completion=cot_completion,\n",
    "            step_delimiter=step_delimiter\n",
    "        )\n",
    "        \n",
    "        # Step 2: Extract features\n",
    "        print(\"\\n[2/5] Extracting features from graphs...\")\n",
    "        step_features = []\n",
    "        for step in reasoning_steps:\n",
    "            if step.graph:\n",
    "                features = self.feature_extractor.extract_all_features(step.graph)\n",
    "                step_features.append(features)\n",
    "        \n",
    "        # Step 3: Analyze sequences\n",
    "        print(\"\\n[3/5] Analyzing sequential patterns...\")\n",
    "        feature_trajectories = self.analyzer.track_feature_evolution(reasoning_steps)\n",
    "        persistent_features = self.analyzer.identify_persistent_features(reasoning_steps)\n",
    "        transitions = self.analyzer.detect_reasoning_transitions(reasoning_steps)\n",
    "        aggregated_features = self.analyzer.aggregate_step_features(reasoning_steps)\n",
    "        \n",
    "        # Step 4: Visualize\n",
    "        if save_visualizations:\n",
    "            print(\"\\n[4/5] Generating visualizations...\")\n",
    "            self.visualizer.plot_feature_trajectories(\n",
    "                feature_trajectories,\n",
    "                save_path=Path(\"feature_trajectories.png\")\n",
    "            )\n",
    "        \n",
    "        # Step 5: Generate report\n",
    "        if generate_report:\n",
    "            print(\"\\n[5/5] Generating report...\")\n",
    "            report = self.visualizer.generate_feature_report(reasoning_steps, self.analyzer)\n",
    "            print(report)\n",
    "        \n",
    "        # Return all results\n",
    "        results = {\n",
    "            'reasoning_steps': reasoning_steps,\n",
    "            'step_features': step_features,\n",
    "            'feature_trajectories': feature_trajectories,\n",
    "            'persistent_features': persistent_features,\n",
    "            'transitions': transitions,\n",
    "            'aggregated_features': aggregated_features,\n",
    "        }\n",
    "        \n",
    "        print(\"\\n✓ Analysis complete!\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47744e12-8762-469a-9e3f-ff9cb313436a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f68b31a424e42d499f615dddf7b6494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b322677b520497cb1498a075fbd68e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb344668a434079ba2d5be34df01e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53259c4234264887827955a36e1f03bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be05608409124b31861c9f9cd2308417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dafdb3aad64272a65d36ffee916866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91228818dafd4fc7a8b7910ba1dbfe9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cc66bad4b34a53af57a005e90a746d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0e96141a3e4927b382d81f090fb9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e83cbe33f54d569b1acf470023861c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd76f018fc4c44458bb1d8e7b72ca638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n",
      "Starting CoT Mechanistic Analysis...\n",
      "\n",
      "[1/5] Generating attribution graphs...\n",
      "\n",
      "============================================================\n",
      "Generating graph for Step 0\n",
      "Step text: Let me solve this step by step....\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputation completed in 0.74s\n",
      "Found 16904 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.23s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7031\n",
      "Will include 4096 of 16904 feature nodes\n",
      "Input vectors built in 0.30s\n",
      "Phase 3: Computing logit attributions\n",
      "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.20s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 4096/4096 [00:02<00:00, 1866.61it/s]\n",
      "Feature attributions completed in 2.20s\n",
      "Attribution completed in 4.29s\n",
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.08s\n",
      "Found 38569 active features\n",
      "Phase 1: Running forward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Generating graph for Step 1\n",
      "Step text: Step 1: First, I'll add the ones place: 5 + 7 = 12...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward pass completed in 0.43s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.9062\n",
      "Will include 4096 of 38569 feature nodes\n",
      "Input vectors built in 0.38s\n",
      "Phase 3: Computing logit attributions\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacity of 31.37 GiB of which 5.41 GiB is free. Including non-PyTorch memory, this process has 25.95 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 5.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mexample_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mexample_usage\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m     cot_completion = \u001b[33m\"\"\"\u001b[39m\u001b[33mLet me solve this step by step.\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33mStep 1: First, I\u001b[39m\u001b[33m'\u001b[39m\u001b[33mll add the ones place: 5 + 7 = 12\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[33mStep 2: I write down 2 and carry the 1\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[33mStep 3: Now add the tens place: 1 + 2 + 1 (carried) = 4\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[33mStep 4: Therefore, 15 + 27 = 42\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Run complete analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     results = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_cot_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcot_completion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcot_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep_delimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgenerate_report\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_visualizations\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Access specific results\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPersistent Features:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mCoTMechanisticAnalyzer.analyze_cot_sequence\u001b[39m\u001b[34m(self, prompt, cot_completion, step_delimiter, generate_report, save_visualizations)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Step 1: Generate graphs\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[1/5] Generating attribution graphs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m reasoning_steps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_sequential_graphs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcot_completion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcot_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_delimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_delimiter\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Step 2: Extract features\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[2/5] Extracting features from graphs...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mCoTAttributionGenerator.generate_sequential_graphs\u001b[39m\u001b[34m(self, prompt, cot_completion, step_delimiter)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(steps):\n\u001b[32m    142\u001b[39m     previous_steps = steps[:i] \u001b[38;5;28;01mif\u001b[39;00m i > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     step.graph = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_graph_for_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreasoning_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprevious_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprevious_steps\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m steps\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mCoTAttributionGenerator.generate_graph_for_step\u001b[39m\u001b[34m(self, prompt, reasoning_step, previous_steps)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Generate attribution graph using circuit-tracer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m graph = \u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/circuit_tracer/attribution/attribute.py:137\u001b[39m, in \u001b[36mattribute\u001b[39m\u001b[34m(prompt, model, max_n_logits, desired_logit_prob, batch_size, max_feature_nodes, offload, verbose, update_interval)\u001b[39m\n\u001b[32m    135\u001b[39m offload_handles = []\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_handles\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_handles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m reload_handle \u001b[38;5;129;01min\u001b[39;00m offload_handles:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/circuit_tracer/attribution/attribute.py:235\u001b[39m, in \u001b[36m_run_attribution\u001b[39m\u001b[34m(model, prompt, max_n_logits, desired_logit_prob, batch_size, max_feature_nodes, offload, verbose, offload_handles, logger, update_interval)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(logit_idx), batch_size):\n\u001b[32m    234\u001b[39m     batch = logit_vecs[i : i + batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     rows = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_pos\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43minject_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m     edge_matrix[i : i + batch.shape[\u001b[32m0\u001b[39m], :logit_offset] = rows.cpu()\n\u001b[32m    241\u001b[39m     row_to_node_index[i : i + batch.shape[\u001b[32m0\u001b[39m]] = (\n\u001b[32m    242\u001b[39m         torch.arange(i, i + batch.shape[\u001b[32m0\u001b[39m]) + logit_offset\n\u001b[32m    243\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/circuit_tracer/attribution/context.py:225\u001b[39m, in \u001b[36mAttributionContext.compute_batch\u001b[39m\u001b[34m(self, layers, positions, inject_values, retain_graph)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    224\u001b[39m     last_layer = \u001b[38;5;28mmax\u001b[39m(layers_in_batch)\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resid_activations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlast_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resid_activations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlast_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handles:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/hooks.py:137\u001b[39m, in \u001b[36mBackwardHook._set_user_hook.<locals>.hook\u001b[39m\u001b[34m(grad_input, _)\u001b[39m\n\u001b[32m    134\u001b[39m res = \u001b[38;5;28mself\u001b[39m._pack_with_none(\u001b[38;5;28mself\u001b[39m.input_tensors_index, grad_input, \u001b[38;5;28mself\u001b[39m.n_inputs)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.user_hooks:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     out = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    140\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformer_lens/hook_points.py:100\u001b[39m, in \u001b[36mHookPoint.add_hook.<locals>.full_hook\u001b[39m\u001b[34m(module, module_input, module_output)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mdir\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mbwd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[32m     99\u001b[39m     module_output = module_output[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/circuit_tracer/attribution/context.py:108\u001b[39m, in \u001b[36mAttributionContext._compute_score_hook.<locals>._hook_fn\u001b[39m\u001b[34m(grads, hook)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_hook_fn\u001b[39m(grads: torch.Tensor, hook: HookPoint) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    107\u001b[39m     proxy._batch_buffer[write_index] += einsum(\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m         \u001b[43mgrads\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_vecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mread_index\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m    109\u001b[39m         output_vecs,\n\u001b[32m    110\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbatch position d_model, position d_model -> position batch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    111\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacity of 31.37 GiB of which 5.41 GiB is free. Including non-PyTorch memory, this process has 25.95 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 5.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the complete pipeline\"\"\"\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = CoTMechanisticAnalyzer(model)\n",
    "    \n",
    "    # Example CoT reasoning\n",
    "    prompt = \"What is 15 + 27?\"\n",
    "    cot_completion = \"\"\"Let me solve this step by step.\n",
    "Step 1: First, I'll add the ones place: 5 + 7 = 12\n",
    "Step 2: I write down 2 and carry the 1\n",
    "Step 3: Now add the tens place: 1 + 2 + 1 (carried) = 4\n",
    "Step 4: Therefore, 15 + 27 = 42\"\"\"\n",
    "    \n",
    "    # Run complete analysis\n",
    "    results = analyzer.analyze_cot_sequence(\n",
    "        prompt=prompt,\n",
    "        cot_completion=cot_completion,\n",
    "        step_delimiter=\"\\n\",\n",
    "        generate_report=True,\n",
    "        save_visualizations=True\n",
    "    )\n",
    "    \n",
    "    # Access specific results\n",
    "    print(\"\\nPersistent Features:\")\n",
    "    for feat_id, score in results['persistent_features'][:5]:\n",
    "        print(f\"  {feat_id}: {score:.2%}\")\n",
    "    \n",
    "    print(\"\\nTransition Points:\")\n",
    "    print(results['transitions'])\n",
    "    \n",
    "    # Compare with non-CoT\n",
    "    nocot_completion = \"42\"\n",
    "    nocot_graph = analyzer.graph_generator.generate_graph_for_step(\n",
    "        prompt=prompt,\n",
    "        reasoning_step=ReasoningStep(0, nocot_completion, 0, 1)\n",
    "    )\n",
    "    \n",
    "    comparison = analyzer.analyzer.compare_cot_vs_nocot(\n",
    "        results['reasoning_steps'],\n",
    "        nocot_graph\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCoT vs Non-CoT Comparison:\")\n",
    "    for metric, value in list(comparison.items())[:5]:\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5e90d-11e2-455b-af11-b630b25dbf4a",
   "metadata": {},
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd3069b-9807-4a2a-ba95-4006b9dc4f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db444bdc33c24528bfbb315593242a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179e5de5ea604fae8146e4d0d940dce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5779e709597743b2b98084f2a6870a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8fb48f464f48eba1685b598ce6b71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd33fdf099545a5b6d8944d9edf743d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb929e856cc3434eae4cf8f5ee8e85b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19c037297e94a5f933fe8c3db348cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01fc8d9a399451fa6ff4c097cb9490f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf6524a955540d78c1e7b4b148fc313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495f3cebd49444c2bd76b7f0aef21e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842254718f424e519d0b6f6b85fc3172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e715ee-a99b-40ac-8c61-193599742a7e",
   "metadata": {},
   "source": [
    "#### Example 1 - Multi-hop reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51da0713-078b-4855-ba74-0070f2e4bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The capital of state containing Dallas is\"  # What you want to get the graph for\n",
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=256  # Batch size when attributing\n",
    "offload= None #'disk' if IN_COLAB else 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2a44b0c-3fd1-4517-8ba2-264badcd8d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.13s\n",
      "Found 6347 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.08s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7188\n",
      "Will include 6347 of 6347 feature nodes\n",
      "Input vectors built in 0.02s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.10s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 6347/6347 [00:02<00:00, 2181.03it/s]\n",
      "Feature attributions completed in 3.10s\n",
      "Attribution completed in 4.03s\n"
     ]
    }
   ],
   "source": [
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e430619f-d17c-479c-8edd-507669961452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<circuit_tracer.graph.Graph at 0x79439f6f1b80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7281b7a8-b6eb-40ae-88ce-69883eefa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'example_graph.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c41761c8-3cd3-438c-9a47-6756751a37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"dallas-austin\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graph_files'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45deaeb3-9500-4ef3-8c3a-4969230016de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the IFrame below, or open your graph here: f'http://localhost:23/index.html'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"http://localhost:23/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7efb463efc80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "\n",
    "port = 23\n",
    "server = serve(data_dir='./graph_files/', port=port)\n",
    "\n",
    "IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import output as colab_output  # noqa\n",
    "    colab_output.serve_kernel_port_as_iframe(port, path='/index.html', height='800px', cache_in_notebook=True)\n",
    "else:\n",
    "    from IPython.display import IFrame\n",
    "    print(f\"Use the IFrame below, or open your graph here: f'http://localhost:{port}/index.html'\")\n",
    "    display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2efde78d-d4e1-4add-82c5-ef28d0c78558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = prune_graph(graph, node_threshold=0.7, edge_threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82240886-d786-48ac-965b-ddd51e44b6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PruneResult(node_mask=tensor([False,  True, False,  ...,  True,  True,  True]), edge_mask=tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False,  True, False,  ..., False, False, False]]), cumulative_scores=tensor([0.7486, 0.6564, 0.8938,  ..., 1.0000, 1.0000, 1.0000]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c801a-74de-4859-8bdd-d7a264d99754",
   "metadata": {},
   "source": [
    "#### Example 2 - GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1834cd0a-e299-4c07-895a-69234700b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?\"  \n",
    "max_n_logits = 10 \n",
    "desired_logit_prob = 0.95\n",
    "max_feature_nodes = 8192\n",
    "batch_size=256\n",
    "offload= None\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e44152-3bca-4c66-93d1-ea90f1fc4c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.47s\n",
      "Found 25788 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.31s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.8672\n",
      "Will include 8192 of 25788 feature nodes\n",
      "Input vectors built in 0.29s\n",
      "Phase 3: Computing logit attributions\n",
      "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.32s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:08<00:00, 1021.06it/s]\n",
      "Feature attributions completed in 8.03s\n",
      "Attribution completed in 9.77s\n"
     ]
    }
   ],
   "source": [
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b4a567-7504-4e14-8adb-fdc199103355",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'example_graph_gsm8k.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a23782-4a85-4852-be3a-ee6657007406",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"gsm8k-james-writes\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graphs'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86a5b4-cdaf-411b-b4b6-9c498c803ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
