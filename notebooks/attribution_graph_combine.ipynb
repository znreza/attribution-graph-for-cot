{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bda2a-7668-4b85-a7c2-0ce7165f5e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Install packages\n",
    "# ========================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "cache_dir = \"/workspace/huggingface_cache\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Hugging Face cache directory is now set to: {os.environ['HF_HOME']}\")\n",
    "\n",
    "!pip install huggingface_hub transformers accelerate einops hf_transfer\n",
    "\n",
    "#  Clone the repository if it doesn't exist, or pull the latest changes if it does.\n",
    "!if [ -d \"repository/circuit-tracer\" ]; then \\\n",
    "    echo \"✅ Repository found. Pulling latest changes...\"; \\\n",
    "    (cd repository/circuit-tracer && git pull); \\\n",
    "else \\\n",
    "    echo \"Cloning repository for the first time...\"; \\\n",
    "    mkdir -p repository && git clone https://github.com/safety-research/circuit-tracer repository/circuit-tracer; \\\n",
    "fi\n",
    "\n",
    "!pip install ./repository/circuit-tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23122025-4dcf-45d1-ab08-fa0a08e4e84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9f08b55c4245179e1a7f6ca5e159b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Add the cloned repository to the Python path\n",
    "sys.path.append('repository/circuit-tracer')\n",
    "sys.path.append('repository/circuit-tracer/demos')\n",
    "\n",
    "# This will prompt you for your token\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28780e7a-562c-457d-b165-183e42e3ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sequential CoT Attribution Analysis\n",
    "Implementation for analyzing Chain-of-Thought reasoning using attribution graphs\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "from circuit_tracer import attribute, ReplacementModel\n",
    "from circuit_tracer.graph import Graph\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "from circuit_tracer.graph import prune_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d965df74-c4c9-45c6-8293-ac2a43d8e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Generate Attribution Graphs for Reasoning Steps\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ReasoningStep:\n",
    "    \"\"\"Represents a single reasoning step in a CoT sequence\"\"\"\n",
    "    step_idx: int\n",
    "    text: str\n",
    "    start_token_idx: int\n",
    "    end_token_idx: int\n",
    "    graph: Optional[Graph] = None\n",
    "\n",
    "\n",
    "class CoTAttributionGenerator:\n",
    "    \"\"\"Generate attribution graphs for each step in a CoT reasoning sequence\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ReplacementModel,\n",
    "        max_n_logits: int = 10,\n",
    "        desired_logit_prob: float = 0.95,\n",
    "        batch_size: int = 512,\n",
    "        max_feature_nodes: int = 4096,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.max_n_logits = max_n_logits\n",
    "        self.desired_logit_prob = desired_logit_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.max_feature_nodes = max_feature_nodes\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def parse_cot_steps(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        cot_completion: str,\n",
    "        step_delimiter: str = \"\\n\"\n",
    "    ) -> List[ReasoningStep]:\n",
    "        \"\"\"\n",
    "        Parse CoT completion into individual reasoning steps\n",
    "        \n",
    "        Args:\n",
    "            prompt: The initial prompt/question\n",
    "            cot_completion: The full CoT reasoning text\n",
    "            step_delimiter: How to split steps (default: newline)\n",
    "        \n",
    "        Returns:\n",
    "            List of ReasoningStep objects\n",
    "        \"\"\"\n",
    "        # Tokenize to get token indices\n",
    "        full_text = prompt + cot_completion\n",
    "        tokens = self.model.tokenizer.encode(full_text)\n",
    "        prompt_tokens = self.model.tokenizer.encode(prompt)\n",
    "        \n",
    "        # Split completion into steps\n",
    "        steps_text = cot_completion.split(step_delimiter)\n",
    "        steps_text = [s.strip() for s in steps_text if s.strip()]\n",
    "        \n",
    "        reasoning_steps = []\n",
    "        current_token_idx = len(prompt_tokens)\n",
    "        \n",
    "        for step_idx, step_text in enumerate(steps_text):\n",
    "            step_tokens = self.model.tokenizer.encode(step_text)\n",
    "            end_token_idx = current_token_idx + len(step_tokens)\n",
    "            \n",
    "            step = ReasoningStep(\n",
    "                step_idx=step_idx,\n",
    "                text=step_text,\n",
    "                start_token_idx=current_token_idx,\n",
    "                end_token_idx=end_token_idx\n",
    "            )\n",
    "            reasoning_steps.append(step)\n",
    "            current_token_idx = end_token_idx\n",
    "        \n",
    "        return reasoning_steps\n",
    "    \n",
    "    def generate_graph_for_step(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        reasoning_step: ReasoningStep,\n",
    "        previous_steps: Optional[List[ReasoningStep]] = None\n",
    "    ) -> Graph:\n",
    "        \"\"\"\n",
    "        Generate attribution graph for a single reasoning step\n",
    "        \n",
    "        Args:\n",
    "            prompt: The initial prompt\n",
    "            reasoning_step: The step to analyze\n",
    "            previous_steps: Previous steps for context\n",
    "        \n",
    "        Returns:\n",
    "            Attribution graph for this step\n",
    "        \"\"\"\n",
    "        # Build context: prompt + all previous steps + current step\n",
    "        context_text = prompt\n",
    "        if previous_steps:\n",
    "            context_text += \" \" + \" \".join([s.text for s in previous_steps])\n",
    "        context_text += \" \" + reasoning_step.text\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Generating graph for Step {reasoning_step.step_idx}\")\n",
    "            print(f\"Step text: {reasoning_step.text[:100]}...\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        # Generate attribution graph using circuit-tracer\n",
    "        graph = attribute(\n",
    "            prompt=context_text,\n",
    "            model=self.model,\n",
    "            max_n_logits=self.max_n_logits,\n",
    "            desired_logit_prob=self.desired_logit_prob,\n",
    "            batch_size=self.batch_size,\n",
    "            max_feature_nodes=self.max_feature_nodes,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def generate_sequential_graphs(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        cot_completion: str,\n",
    "        step_delimiter: str = \"\\n\"\n",
    "    ) -> List[ReasoningStep]:\n",
    "        \"\"\"\n",
    "        Generate attribution graphs for all steps in a CoT sequence\n",
    "        \n",
    "        Args:\n",
    "            prompt: The initial prompt\n",
    "            cot_completion: The full CoT reasoning\n",
    "            step_delimiter: How to split steps\n",
    "        \n",
    "        Returns:\n",
    "            List of ReasoningSteps with graphs attached\n",
    "        \"\"\"\n",
    "        # Parse steps\n",
    "        steps = self.parse_cot_steps(prompt, cot_completion, step_delimiter)\n",
    "        \n",
    "        # Generate graph for each step\n",
    "        for i, step in enumerate(steps):\n",
    "            previous_steps = steps[:i] if i > 0 else None\n",
    "            step.graph = self.generate_graph_for_step(\n",
    "                prompt=prompt,\n",
    "                reasoning_step=step,\n",
    "                previous_steps=previous_steps\n",
    "            )\n",
    "        \n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50a5904-8588-46da-809c-12bd084c6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Aggregate and Combine Attribution Graphs\n",
    "# based on https://github.com/safety-research/circuit-tracer/blob/main/circuit_tracer/graph.py\n",
    "# ============================================================================\n",
    "\n",
    "class GraphAggregator:\n",
    "    \"\"\"Combine and aggregate multiple attribution graphs\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_sequential_graphs(\n",
    "        reasoning_steps: List[ReasoningStep],\n",
    "        merge_strategy: str = \"union\"\n",
    "    ) -> Graph:\n",
    "        \"\"\"\n",
    "        Merge multiple graphs into a single combined graph\n",
    "        \n",
    "        Args:\n",
    "            reasoning_steps: Steps with graphs to merge\n",
    "            merge_strategy: 'union' (combine all) or 'intersection' (common only)\n",
    "        \n",
    "        Returns:\n",
    "            Merged graph\n",
    "        \"\"\"\n",
    "        if not reasoning_steps or not reasoning_steps[0].graph:\n",
    "            raise ValueError(\"No graphs to merge\")\n",
    "        \n",
    "        # Start with first graph\n",
    "        merged_graph = reasoning_steps[0].graph\n",
    "        \n",
    "        if merge_strategy == \"union\":\n",
    "            # Combine all nodes and edges\n",
    "            for step in reasoning_steps[1:]:\n",
    "                if step.graph:\n",
    "                    merged_graph = GraphAggregator._union_graphs(\n",
    "                        merged_graph, \n",
    "                        step.graph\n",
    "                    )\n",
    "        \n",
    "        elif merge_strategy == \"intersection\":\n",
    "            # Keep only common nodes/edges\n",
    "            for step in reasoning_steps[1:]:\n",
    "                if step.graph:\n",
    "                    merged_graph = GraphAggregator._intersect_graphs(\n",
    "                        merged_graph, \n",
    "                        step.graph\n",
    "                    )\n",
    "        \n",
    "        return merged_graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def _union_graphs(graph1: Graph, graph2: Graph) -> Graph:\n",
    "        \"\"\"Combine two graphs (union of nodes and edges)\"\"\"\n",
    "        # This is a simplified implementation\n",
    "        \n",
    "        combined_nodes = set(graph1.nodes) | set(graph2.nodes)\n",
    "        combined_edges = {}\n",
    "        \n",
    "        # Combine edges, summing weights for common edges\n",
    "        for edge, weight in graph1.edges.items():\n",
    "            combined_edges[edge] = weight\n",
    "        \n",
    "        for edge, weight in graph2.edges.items():\n",
    "            if edge in combined_edges:\n",
    "                combined_edges[edge] += weight\n",
    "            else:\n",
    "                combined_edges[edge] = weight\n",
    "        \n",
    "        # Create new graph (THIS PART NEEDS TO BE VALIDATED)\n",
    "        \n",
    "        return Graph(nodes=combined_nodes, edges=combined_edges)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _intersect_graphs(graph1: Graph, graph2: Graph) -> Graph:\n",
    "        \"\"\"Find common elements between two graphs\"\"\"\n",
    "        common_nodes = set(graph1.nodes) & set(graph2.nodes)\n",
    "        common_edges = {}\n",
    "        \n",
    "        # Keep only edges present in both\n",
    "        for edge, weight1 in graph1.edges.items():\n",
    "            if edge in graph2.edges:\n",
    "                # Average the weights\n",
    "                weight2 = graph2.edges[edge]\n",
    "                common_edges[edge] = (weight1 + weight2) / 2\n",
    "        \n",
    "        return Graph(nodes=common_nodes, edges=common_edges)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_graph_similarity(\n",
    "        graph1: Graph, \n",
    "        graph2: Graph,\n",
    "        method: str = \"jaccard\"\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity between two graphs\n",
    "        \n",
    "        Args:\n",
    "            graph1, graph2: Graphs to compare\n",
    "            method: 'jaccard' (node overlap) or 'edge_overlap'\n",
    "        \n",
    "        Returns:\n",
    "            Similarity score [0, 1]\n",
    "        \"\"\"\n",
    "        if method == \"jaccard\":\n",
    "            nodes1 = set(graph1.nodes)\n",
    "            nodes2 = set(graph2.nodes)\n",
    "            intersection = len(nodes1 & nodes2)\n",
    "            union = len(nodes1 | nodes2)\n",
    "            return intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        elif method == \"edge_overlap\":\n",
    "            edges1 = set(graph1.edges.keys())\n",
    "            edges2 = set(graph2.edges.keys())\n",
    "            intersection = len(edges1 & edges2)\n",
    "            union = len(edges1 | edges2)\n",
    "            return intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f09fcb-380f-4d58-b76c-f2221897a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Extract Features from Attribution Graphs\n",
    "# based on https://github.com/safety-research/circuit-tracer/blob/main/circuit_tracer/graph.py\n",
    "# ============================================================================\n",
    "\n",
    "class GraphFeatureExtractor:\n",
    "    \"\"\"Extract interpretable features from attribution graphs\"\"\"\n",
    "    \n",
    "    def __init__(self, model: ReplacementModel):\n",
    "        self.model = model\n",
    "        self.transcoder = model.transcoder if hasattr(model, 'transcoder') else None\n",
    "    \n",
    "    def extract_all_features(self, graph: Graph) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Extract comprehensive feature set from a graph\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all extracted features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Global graph statistics\n",
    "        features.update(self.extract_global_statistics(graph))\n",
    "        \n",
    "        # Node-level features\n",
    "        features.update(self.extract_node_features(graph))\n",
    "        \n",
    "        # Topological features\n",
    "        features.update(self.extract_topological_features(graph))\n",
    "        \n",
    "        # Transcoder-specific features\n",
    "        if self.transcoder:\n",
    "            features.update(self.extract_transcoder_features(graph))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_global_statistics(self, graph: Graph) -> Dict[str, float]:\n",
    "        \"\"\"Extract global graph-level statistics\"\"\"\n",
    "        features = {\n",
    "            'num_nodes': len(graph.nodes),\n",
    "            'num_edges': len(graph.edges),\n",
    "            'num_feature_nodes': sum(1 for n in graph.nodes if n.type == 'feature'),\n",
    "            'num_token_nodes': sum(1 for n in graph.nodes if n.type == 'token'),\n",
    "            'num_logit_nodes': sum(1 for n in graph.nodes if n.type == 'logit'),\n",
    "        }\n",
    "        \n",
    "        # Logit statistics\n",
    "        if hasattr(graph, 'logit_probs'):\n",
    "            features['top_logit_prob'] = max(graph.logit_probs.values())\n",
    "            features['logit_entropy'] = self._compute_entropy(\n",
    "                list(graph.logit_probs.values())\n",
    "            )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_node_features(self, graph: Graph) -> Dict[str, float]:\n",
    "        \"\"\"Extract node-level statistics\"\"\"\n",
    "        activations = []\n",
    "        influences = []\n",
    "        \n",
    "        for node in graph.nodes:\n",
    "            if hasattr(node, 'activation'):\n",
    "                activations.append(node.activation)\n",
    "            if hasattr(node, 'influence'):\n",
    "                influences.append(node.influence)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        if activations:\n",
    "            features['mean_activation'] = np.mean(activations)\n",
    "            features['max_activation'] = np.max(activations)\n",
    "            features['std_activation'] = np.std(activations)\n",
    "        \n",
    "        if influences:\n",
    "            features['mean_influence'] = np.mean(influences)\n",
    "            features['max_influence'] = np.max(influences)\n",
    "            features['total_influence'] = np.sum(influences)\n",
    "        \n",
    "        # Layer-wise histogram\n",
    "        layer_counts = defaultdict(int)\n",
    "        for node in graph.nodes:\n",
    "            if hasattr(node, 'layer'):\n",
    "                layer_counts[node.layer] += 1\n",
    "        \n",
    "        # Convert to feature vector\n",
    "        max_layers = 32  # Adjust based on model\n",
    "        for layer in range(max_layers):\n",
    "            features[f'layer_{layer}_count'] = layer_counts.get(layer, 0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_topological_features(self, graph: Graph) -> Dict[str, float]:\n",
    "        \"\"\"Extract graph topology features\"\"\"\n",
    "        # Convert to NetworkX for analysis\n",
    "        G = self._to_networkx(graph)\n",
    "        \n",
    "        features = {\n",
    "            'graph_density': nx.density(G),\n",
    "            'num_connected_components': nx.number_weakly_connected_components(G),\n",
    "        }\n",
    "        \n",
    "        # Edge statistics\n",
    "        edge_weights = [data.get('weight', 1.0) for _, _, data in G.edges(data=True)]\n",
    "        if edge_weights:\n",
    "            features['mean_edge_weight'] = np.mean(edge_weights)\n",
    "            features['max_edge_weight'] = np.max(edge_weights)\n",
    "            features['sum_edge_weights'] = np.sum(edge_weights)\n",
    "        \n",
    "        # Centrality measures\n",
    "        try:\n",
    "            degree_centrality = nx.degree_centrality(G)\n",
    "            features['mean_degree_centrality'] = np.mean(list(degree_centrality.values()))\n",
    "            features['max_degree_centrality'] = np.max(list(degree_centrality.values()))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Path-based features\n",
    "        try:\n",
    "            # Shortest paths from input to output\n",
    "            input_nodes = [n for n in G.nodes() if 'input' in str(n)]\n",
    "            output_nodes = [n for n in G.nodes() if 'output' in str(n)]\n",
    "            \n",
    "            if input_nodes and output_nodes:\n",
    "                paths = []\n",
    "                for inp in input_nodes[:5]:  # Sample a few\n",
    "                    for out in output_nodes[:5]:\n",
    "                        try:\n",
    "                            length = nx.shortest_path_length(G, inp, out)\n",
    "                            paths.append(length)\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                if paths:\n",
    "                    features['mean_path_length'] = np.mean(paths)\n",
    "                    features['min_path_length'] = np.min(paths)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_transcoder_features(self, graph: Graph) -> Dict[str, float]:\n",
    "        \"\"\"Extract features specific to transcoder activations\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Feature sparsity\n",
    "        active_features = [n for n in graph.nodes if n.type == 'feature' and n.activation > 0]\n",
    "        features['num_active_features'] = len(active_features)\n",
    "        features['feature_sparsity'] = len(active_features) / max(len(graph.nodes), 1)\n",
    "        \n",
    "        # Feature activation patterns by layer\n",
    "        layer_activations = defaultdict(list)\n",
    "        for node in active_features:\n",
    "            if hasattr(node, 'layer'):\n",
    "                layer_activations[node.layer].append(node.activation)\n",
    "        \n",
    "        for layer, acts in layer_activations.items():\n",
    "            features[f'layer_{layer}_mean_activation'] = np.mean(acts)\n",
    "            features[f'layer_{layer}_num_active'] = len(acts)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_entropy(probs: List[float]) -> float:\n",
    "        \"\"\"Compute Shannon entropy\"\"\"\n",
    "        probs = np.array(probs)\n",
    "        probs = probs / probs.sum()  # Normalize\n",
    "        return -np.sum(probs * np.log2(probs + 1e-10))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_networkx(graph: Graph) -> nx.DiGraph:\n",
    "        \"\"\"Convert Graph to NetworkX for analysis\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for node in graph.nodes:\n",
    "            G.add_node(\n",
    "                node.id,\n",
    "                type=node.type,\n",
    "                activation=getattr(node, 'activation', 0),\n",
    "                influence=getattr(node, 'influence', 0)\n",
    "            )\n",
    "        \n",
    "        # Add edges\n",
    "        for (src, tgt), weight in graph.edges.items():\n",
    "            G.add_edge(src, tgt, weight=weight)\n",
    "        \n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ba33fb-9651-40e4-b59b-7ffda7939010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Advanced Analysis Functions\n",
    "# ============================================================================\n",
    "\n",
    "class SequentialFeatureAnalyzer:\n",
    "    \"\"\"Analyze features across multiple reasoning steps\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_extractor: GraphFeatureExtractor):\n",
    "        self.extractor = feature_extractor\n",
    "    \n",
    "    def track_feature_evolution(\n",
    "        self,\n",
    "        reasoning_steps: List[ReasoningStep]\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Track how features change across reasoning steps\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping feature names to time series\n",
    "        \"\"\"\n",
    "        feature_trajectories = defaultdict(list)\n",
    "        \n",
    "        for step in reasoning_steps:\n",
    "            if step.graph:\n",
    "                features = self.extractor.extract_all_features(step.graph)\n",
    "                for feat_name, feat_value in features.items():\n",
    "                    feature_trajectories[feat_name].append(feat_value)\n",
    "        \n",
    "        return dict(feature_trajectories)\n",
    "    \n",
    "    def identify_persistent_features(\n",
    "        self,\n",
    "        reasoning_steps: List[ReasoningStep],\n",
    "        threshold: float = 0.5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Identify features that persist across multiple steps\n",
    "        \n",
    "        Args:\n",
    "            reasoning_steps: Steps to analyze\n",
    "            threshold: Minimum proportion of steps where feature must appear\n",
    "        \n",
    "        Returns:\n",
    "            List of (feature_id, persistence_score) tuples\n",
    "        \"\"\"\n",
    "        feature_presence = defaultdict(int)\n",
    "        total_steps = len(reasoning_steps)\n",
    "        \n",
    "        for step in reasoning_steps:\n",
    "            if step.graph:\n",
    "                active_features = set(\n",
    "                    n.id for n in step.graph.nodes \n",
    "                    if n.type == 'feature' and n.activation > 0\n",
    "                )\n",
    "                for feat_id in active_features:\n",
    "                    feature_presence[feat_id] += 1\n",
    "        \n",
    "        # Calculate persistence scores\n",
    "        persistent_features = [\n",
    "            (feat_id, count / total_steps)\n",
    "            for feat_id, count in feature_presence.items()\n",
    "            if count / total_steps >= threshold\n",
    "        ]\n",
    "        \n",
    "        return sorted(persistent_features, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def detect_reasoning_transitions(\n",
    "        self,\n",
    "        reasoning_steps: List[ReasoningStep]\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Detect critical transition points in reasoning\n",
    "        \n",
    "        Returns:\n",
    "            List of step indices where significant changes occur\n",
    "        \"\"\"\n",
    "        if len(reasoning_steps) < 2:\n",
    "            return []\n",
    "        \n",
    "        transitions = []\n",
    "        \n",
    "        for i in range(1, len(reasoning_steps)):\n",
    "            prev_graph = reasoning_steps[i-1].graph\n",
    "            curr_graph = reasoning_steps[i].graph\n",
    "            \n",
    "            if prev_graph and curr_graph:\n",
    "                # Compute graph similarity\n",
    "                similarity = GraphAggregator.compute_graph_similarity(\n",
    "                    prev_graph, \n",
    "                    curr_graph,\n",
    "                    method=\"jaccard\"\n",
    "                )\n",
    "                \n",
    "                # If similarity drops significantly, it's a transition\n",
    "                if similarity < 0.3:  # Threshold\n",
    "                    transitions.append(i)\n",
    "        \n",
    "        return transitions\n",
    "    \n",
    "    def compare_cot_vs_nocot(\n",
    "        self,\n",
    "        cot_steps: List[ReasoningStep],\n",
    "        nocot_graph: Graph\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compare CoT reasoning graphs with non-CoT direct answer\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of comparison metrics\n",
    "        \"\"\"\n",
    "        # Extract features from non-CoT\n",
    "        nocot_features = self.extractor.extract_all_features(nocot_graph)\n",
    "        \n",
    "        # Extract features from each CoT step\n",
    "        cot_feature_series = []\n",
    "        for step in cot_steps:\n",
    "            if step.graph:\n",
    "                features = self.extractor.extract_all_features(step.graph)\n",
    "                cot_feature_series.append(features)\n",
    "        \n",
    "        # Compute statistics\n",
    "        comparison = {}\n",
    "        \n",
    "        # Average CoT features\n",
    "        avg_cot_features = {}\n",
    "        for feat_name in nocot_features.keys():\n",
    "            values = [f.get(feat_name, 0) for f in cot_feature_series]\n",
    "            if values:\n",
    "                avg_cot_features[feat_name] = np.mean(values)\n",
    "        \n",
    "        # Compare\n",
    "        for feat_name in nocot_features.keys():\n",
    "            nocot_val = nocot_features[feat_name]\n",
    "            cot_val = avg_cot_features.get(feat_name, 0)\n",
    "            \n",
    "            if nocot_val != 0:\n",
    "                comparison[f'{feat_name}_ratio'] = cot_val / nocot_val\n",
    "            comparison[f'{feat_name}_diff'] = cot_val - nocot_val\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def aggregate_step_features(\n",
    "        self,\n",
    "        reasoning_steps: List[ReasoningStep],\n",
    "        aggregation: str = \"mean\"\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Aggregate features across all steps\n",
    "        \n",
    "        Args:\n",
    "            reasoning_steps: Steps to aggregate\n",
    "            aggregation: 'mean', 'sum', 'max', 'min'\n",
    "        \n",
    "        Returns:\n",
    "            Aggregated feature dictionary\n",
    "        \"\"\"\n",
    "        all_features = []\n",
    "        for step in reasoning_steps:\n",
    "            if step.graph:\n",
    "                features = self.extractor.extract_all_features(step.graph)\n",
    "                all_features.append(features)\n",
    "        \n",
    "        if not all_features:\n",
    "            return {}\n",
    "        \n",
    "        aggregated = {}\n",
    "        feature_names = all_features[0].keys()\n",
    "        \n",
    "        for feat_name in feature_names:\n",
    "            values = [f.get(feat_name, 0) for f in all_features]\n",
    "            \n",
    "            if aggregation == \"mean\":\n",
    "                aggregated[feat_name] = np.mean(values)\n",
    "            elif aggregation == \"sum\":\n",
    "                aggregated[feat_name] = np.sum(values)\n",
    "            elif aggregation == \"max\":\n",
    "                aggregated[feat_name] = np.max(values)\n",
    "            elif aggregation == \"min\":\n",
    "                aggregated[feat_name] = np.min(values)\n",
    "        \n",
    "        return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be211792-c9ce-4741-aca2-b4610a5f857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Visualization and Reporting\n",
    "# ============================================================================\n",
    "\n",
    "class FeatureVisualizer:\n",
    "    \"\"\"Visualize and report on extracted features\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_feature_trajectories(\n",
    "        feature_trajectories: Dict[str, List[float]],\n",
    "        feature_names: Optional[List[str]] = None,\n",
    "        save_path: Optional[Path] = None\n",
    "    ):\n",
    "        \"\"\"Plot how features evolve across reasoning steps\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if feature_names is None:\n",
    "            # Plot first 10 features\n",
    "            feature_names = list(feature_trajectories.keys())[:10]\n",
    "        \n",
    "        fig, axes = plt.subplots(len(feature_names), 1, figsize=(12, 3*len(feature_names)))\n",
    "        if len(feature_names) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, feat_name in zip(axes, feature_names):\n",
    "            values = feature_trajectories[feat_name]\n",
    "            ax.plot(values, marker='o')\n",
    "            ax.set_title(feat_name)\n",
    "            ax.set_xlabel('Reasoning Step')\n",
    "            ax.set_ylabel('Feature Value')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_feature_report(\n",
    "        reasoning_steps: List[ReasoningStep],\n",
    "        analyzer: SequentialFeatureAnalyzer\n",
    "    ) -> str:\n",
    "        \"\"\"Generate a text report of feature analysis\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\"*60)\n",
    "        report.append(\"CHAIN-OF-THOUGHT FEATURE ANALYSIS REPORT\")\n",
    "        report.append(\"=\"*60)\n",
    "        report.append(f\"\\nTotal Reasoning Steps: {len(reasoning_steps)}\")\n",
    "        \n",
    "        # Persistent features\n",
    "        persistent = analyzer.identify_persistent_features(reasoning_steps)\n",
    "        report.append(f\"\\n--- Persistent Features (appear in >50% of steps) ---\")\n",
    "        for feat_id, score in persistent[:10]:\n",
    "            report.append(f\"  {feat_id}: {score:.2%}\")\n",
    "        \n",
    "        # Transitions\n",
    "        transitions = analyzer.detect_reasoning_transitions(reasoning_steps)\n",
    "        report.append(f\"\\n--- Reasoning Transitions ---\")\n",
    "        report.append(f\"Detected {len(transitions)} significant transitions at steps: {transitions}\")\n",
    "        \n",
    "        # Feature evolution\n",
    "        trajectories = analyzer.track_feature_evolution(reasoning_steps)\n",
    "        report.append(f\"\\n--- Key Feature Trends ---\")\n",
    "        for feat_name in ['num_active_features', 'mean_influence', 'graph_density']:\n",
    "            if feat_name in trajectories:\n",
    "                values = trajectories[feat_name]\n",
    "                report.append(f\"  {feat_name}:\")\n",
    "                report.append(f\"    Start: {values[0]:.4f}, End: {values[-1]:.4f}, Change: {values[-1] - values[0]:.4f}\")\n",
    "        \n",
    "        report.append(\"\\n\" + \"=\"*60)\n",
    "        return \"\\n\".join(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781d6f3e-93f7-4ea4-9566-90419738e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: End-to-End Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "class CoTMechanisticAnalyzer:\n",
    "    \"\"\"Complete pipeline for analyzing CoT reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self, model: ReplacementModel):\n",
    "        self.model = model\n",
    "        self.graph_generator = CoTAttributionGenerator(model)\n",
    "        self.feature_extractor = GraphFeatureExtractor(model)\n",
    "        self.analyzer = SequentialFeatureAnalyzer(self.feature_extractor)\n",
    "        self.visualizer = FeatureVisualizer()\n",
    "    \n",
    "    def analyze_cot_sequence(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        cot_completion: str,\n",
    "        step_delimiter: str = \"\\n\",\n",
    "        generate_report: bool = True,\n",
    "        save_visualizations: bool = False\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete end-to-end analysis of a CoT sequence\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all analysis results\n",
    "        \"\"\"\n",
    "        print(\"Starting CoT Mechanistic Analysis...\")\n",
    "        \n",
    "        # Step 1: Generate graphs\n",
    "        print(\"\\n[1/5] Generating attribution graphs...\")\n",
    "        reasoning_steps = self.graph_generator.generate_sequential_graphs(\n",
    "            prompt=prompt,\n",
    "            cot_completion=cot_completion,\n",
    "            step_delimiter=step_delimiter\n",
    "        )\n",
    "        \n",
    "        # Step 2: Extract features\n",
    "        print(\"\\n[2/5] Extracting features from graphs...\")\n",
    "        step_features = []\n",
    "        for step in reasoning_steps:\n",
    "            if step.graph:\n",
    "                features = self.feature_extractor.extract_all_features(step.graph)\n",
    "                step_features.append(features)\n",
    "        \n",
    "        # Step 3: Analyze sequences\n",
    "        print(\"\\n[3/5] Analyzing sequential patterns...\")\n",
    "        feature_trajectories = self.analyzer.track_feature_evolution(reasoning_steps)\n",
    "        persistent_features = self.analyzer.identify_persistent_features(reasoning_steps)\n",
    "        transitions = self.analyzer.detect_reasoning_transitions(reasoning_steps)\n",
    "        aggregated_features = self.analyzer.aggregate_step_features(reasoning_steps)\n",
    "        \n",
    "        # Step 4: Visualize\n",
    "        if save_visualizations:\n",
    "            print(\"\\n[4/5] Generating visualizations...\")\n",
    "            self.visualizer.plot_feature_trajectories(\n",
    "                feature_trajectories,\n",
    "                save_path=Path(\"feature_trajectories.png\")\n",
    "            )\n",
    "        \n",
    "        # Step 5: Generate report\n",
    "        if generate_report:\n",
    "            print(\"\\n[5/5] Generating report...\")\n",
    "            report = self.visualizer.generate_feature_report(reasoning_steps, self.analyzer)\n",
    "            print(report)\n",
    "        \n",
    "        # Return all results\n",
    "        results = {\n",
    "            'reasoning_steps': reasoning_steps,\n",
    "            'step_features': step_features,\n",
    "            'feature_trajectories': feature_trajectories,\n",
    "            'persistent_features': persistent_features,\n",
    "            'transitions': transitions,\n",
    "            'aggregated_features': aggregated_features,\n",
    "        }\n",
    "        \n",
    "        print(\"\\n✓ Analysis complete!\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47744e12-8762-469a-9e3f-ff9cb313436a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the complete pipeline\"\"\"\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = CoTMechanisticAnalyzer(model)\n",
    "    \n",
    "    # Example CoT reasoning\n",
    "    prompt = \"What is 15 + 27?\"\n",
    "    cot_completion = \"\"\"Let me solve this step by step.\n",
    "Step 1: First, I'll add the ones place: 5 + 7 = 12\n",
    "Step 2: I write down 2 and carry the 1\n",
    "Step 3: Now add the tens place: 1 + 2 + 1 (carried) = 4\n",
    "Step 4: Therefore, 15 + 27 = 42\"\"\"\n",
    "    \n",
    "    # Run complete analysis\n",
    "    results = analyzer.analyze_cot_sequence(\n",
    "        prompt=prompt,\n",
    "        cot_completion=cot_completion,\n",
    "        step_delimiter=\"\\n\",\n",
    "        generate_report=True,\n",
    "        save_visualizations=True\n",
    "    )\n",
    "    \n",
    "    # Access specific results\n",
    "    print(\"\\nPersistent Features:\")\n",
    "    for feat_id, score in results['persistent_features'][:5]:\n",
    "        print(f\"  {feat_id}: {score:.2%}\")\n",
    "    \n",
    "    print(\"\\nTransition Points:\")\n",
    "    print(results['transitions'])\n",
    "    \n",
    "    # Compare with non-CoT\n",
    "    nocot_completion = \"42\"\n",
    "    nocot_graph = analyzer.graph_generator.generate_graph_for_step(\n",
    "        prompt=prompt,\n",
    "        reasoning_step=ReasoningStep(0, nocot_completion, 0, 1)\n",
    "    )\n",
    "    \n",
    "    comparison = analyzer.analyzer.compare_cot_vs_nocot(\n",
    "        results['reasoning_steps'],\n",
    "        nocot_graph\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCoT vs Non-CoT Comparison:\")\n",
    "    for metric, value in list(comparison.items())[:5]:\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5e90d-11e2-455b-af11-b630b25dbf4a",
   "metadata": {},
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd3069b-9807-4a2a-ba95-4006b9dc4f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db444bdc33c24528bfbb315593242a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179e5de5ea604fae8146e4d0d940dce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5779e709597743b2b98084f2a6870a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8fb48f464f48eba1685b598ce6b71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd33fdf099545a5b6d8944d9edf743d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb929e856cc3434eae4cf8f5ee8e85b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19c037297e94a5f933fe8c3db348cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01fc8d9a399451fa6ff4c097cb9490f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf6524a955540d78c1e7b4b148fc313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495f3cebd49444c2bd76b7f0aef21e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842254718f424e519d0b6f6b85fc3172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e715ee-a99b-40ac-8c61-193599742a7e",
   "metadata": {},
   "source": [
    "#### Example 1 - Multi-hop reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51da0713-078b-4855-ba74-0070f2e4bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The capital of state containing Dallas is\"  # What you want to get the graph for\n",
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=256  # Batch size when attributing\n",
    "offload= None #'disk' if IN_COLAB else 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2a44b0c-3fd1-4517-8ba2-264badcd8d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.13s\n",
      "Found 6347 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.08s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7188\n",
      "Will include 6347 of 6347 feature nodes\n",
      "Input vectors built in 0.02s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.10s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 6347/6347 [00:02<00:00, 2181.03it/s]\n",
      "Feature attributions completed in 3.10s\n",
      "Attribution completed in 4.03s\n"
     ]
    }
   ],
   "source": [
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e430619f-d17c-479c-8edd-507669961452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<circuit_tracer.graph.Graph at 0x79439f6f1b80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7281b7a8-b6eb-40ae-88ce-69883eefa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'example_graph.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c41761c8-3cd3-438c-9a47-6756751a37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"dallas-austin\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graph_files'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45deaeb3-9500-4ef3-8c3a-4969230016de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the IFrame below, or open your graph here: f'http://localhost:23/index.html'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"http://localhost:23/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7efb463efc80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "\n",
    "port = 23\n",
    "server = serve(data_dir='./graph_files/', port=port)\n",
    "\n",
    "IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import output as colab_output  # noqa\n",
    "    colab_output.serve_kernel_port_as_iframe(port, path='/index.html', height='800px', cache_in_notebook=True)\n",
    "else:\n",
    "    from IPython.display import IFrame\n",
    "    print(f\"Use the IFrame below, or open your graph here: f'http://localhost:{port}/index.html'\")\n",
    "    display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2efde78d-d4e1-4add-82c5-ef28d0c78558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = prune_graph(graph, node_threshold=0.7, edge_threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82240886-d786-48ac-965b-ddd51e44b6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PruneResult(node_mask=tensor([False,  True, False,  ...,  True,  True,  True]), edge_mask=tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False,  True, False,  ..., False, False, False]]), cumulative_scores=tensor([0.7486, 0.6564, 0.8938,  ..., 1.0000, 1.0000, 1.0000]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c801a-74de-4859-8bdd-d7a264d99754",
   "metadata": {},
   "source": [
    "#### Example 2 - GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1834cd0a-e299-4c07-895a-69234700b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?\"  \n",
    "max_n_logits = 10 \n",
    "desired_logit_prob = 0.95\n",
    "max_feature_nodes = 8192\n",
    "batch_size=256\n",
    "offload= None\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e44152-3bca-4c66-93d1-ea90f1fc4c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.47s\n",
      "Found 25788 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.31s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.8672\n",
      "Will include 8192 of 25788 feature nodes\n",
      "Input vectors built in 0.29s\n",
      "Phase 3: Computing logit attributions\n",
      "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.32s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:08<00:00, 1021.06it/s]\n",
      "Feature attributions completed in 8.03s\n",
      "Attribution completed in 9.77s\n"
     ]
    }
   ],
   "source": [
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b4a567-7504-4e14-8adb-fdc199103355",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'example_graph_gsm8k.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a23782-4a85-4852-be3a-ee6657007406",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"gsm8k-james-writes\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graphs'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86a5b4-cdaf-411b-b4b6-9c498c803ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
