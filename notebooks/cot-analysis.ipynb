{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129ef284-5a58-4adb-9056-28feb65ea1b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9912c37d-c92e-466c-a26e-52abac701c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc419c7cae6b4679a075268a4e6ed7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Add the cloned repository to the Python path\n",
    "sys.path.append('repository/circuit-tracer')\n",
    "sys.path.append('repository/circuit-tracer/demos')\n",
    "\n",
    "# This will prompt you for your token\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416ea4d0-f964-4796-ab1c-6126b31adc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7c973fe-6cea-4c53-8b90-19ea16aa7c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c891624b2d412cbf6cdf1c9354dba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9547a534ba9645f8ae86257f92653470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a72fa6c8abe487894f31101bbc1de78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1d75ebcf4e4168a027b3db5d507618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb96293232549958c72f56a1e03d9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e5f080f7994abc8407f09454a645c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6c03952fac45f9abea0f08a15837f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7263e658411a4bfeae6fa64102cb3093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e58bfcb0f074c13960ac510c5941320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161914ba4eb84b128894605a2b6aba58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb804646da8c48a199717384c6372211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc110bf-2a90-4f17-9d52-d6ac45854a3e",
   "metadata": {},
   "source": [
    "### Example 1 - Multi-hop Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07168007-920c-4327-9ae5-230f1ad118de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>The capital of state containing Dallas is Austin.\n",
      "\n",
      "The capital of state containing Dallas is Austin.\n",
      "\n",
      "The capital of state containing Dallas is Austin.\n",
      "\n",
      "The capital of state containing Dallas is Austin.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The capital of state containing Dallas is\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb7a2740-f36c-4917-b25d-55e8c34f820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's think step by step and answer the following question. Question: The capital of the state containing Dallas is Austin. The capital of the state containing Houston is Austin. The capital of the state containing San Antonio is Austin. The capital of the state containing San Diego is Austin. The capital of the state containing San Francisco is Austin. The capital of\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "  \"Let's think step by step and answer the following question. \"\n",
    "  \"Question: The capital of the state containing Dallas is\"\n",
    ")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "out = model.generate(input_ids, max_new_tokens=48, do_sample=False, temperature=0.0)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999085ca-9c17-4c7b-8060-20ea9164ac53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of the state containing Dallas? Let me solve this step by step.\n",
      "\n",
      "Step 1:\n",
      "\n",
      "The state containing Dallas is Texas.\n",
      "\n",
      "Step 2:\n",
      "\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Step 3:\n",
      "\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Step 4:\n",
      "\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Step 5:\n",
      "\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Step 6:\n",
      "\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Step 7:\n",
      "\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Step 8:\n",
      "\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Step 9:\n",
      "\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Step 10:\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "  \"What is the capital of the state containing Dallas? Let me solve this step by step.\"\n",
    ")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "out = model.generate(input_ids, max_new_tokens=125, do_sample=False, temperature=0.0)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ec4fa-8ab1-4e7e-aebb-b9e668c072d0",
   "metadata": {},
   "source": [
    "### Example 2 - GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75321a63-b788-4d65-877c-4bfb5488e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "\n",
      "A. $60\n",
      "\n",
      "B. $\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\" #Answer: $10\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "def964f2-4397-4550-8341-6cd405b71fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "\n",
      "A. $60\n",
      "\n",
      "B. $62\n",
      "\n",
      "C. $64\n",
      "\n",
      "D. $66\n",
      "\n",
      "Show more\n",
      "Step 1\n",
      "1 of 2\n",
      "\n",
      "$12\\times 50=600 $\n",
      "\n",
      "The amount of money earned is the product of the number of hours and the hourly rate.\n",
      "\n",
      "Result\n",
      "2 of 2\n",
      "\n",
      "A<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\" #Answer: $10\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=125)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b484a96-24f0-4859-a758-ebf2d5228af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Let's think step by step and answer the following question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "\n",
      "Step 1\n",
      "1 of 2\n",
      "\n",
      "Let $x$ be the number of hours Weng worked.\n",
      "\n",
      "The amount of money she earned is the product of the number of hours she worked and the hourly rate.\n",
      "\n",
      "$\\begin{align*} \\text{amount of money earned} &= \\text{number of hours worked} \\times \\text{hourly rate} \\\\ &= x \\times \\$12 \\\\ &= \\$12x \\end{align*} $\n",
      "\n",
      "The amount of money she earned is $\\$12x$.\n",
      "\n",
      "Result\n",
      "2 of 2\n",
      "\n",
      "$\\$12x $<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Let's think step by step and answer the following question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\" #Answer: $10\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=125)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62095b72-3ee4-4ff8-b778-2f3998dbf0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year? Let me solve this step by step.\n",
      "\n",
      "Step 1: Find the number of weeks in a year.\n",
      "\n",
      "There are 52 weeks in a year.\n",
      "\n",
      "Step 2: Find the number of letters James writes in a year.\n",
      "\n",
      "James writes 2 letters a week.\n",
      "\n",
      "So, he writes 2 x 52 = 104 letters in a year.\n",
      "\n",
      "Step 3: Find the number of pages James writes in a year.\n",
      "\n",
      "James writes 3 pages in a letter.\n",
      "\n",
      "So, he writes 3 x 104 = 312 pages in a year.\n",
      "\n",
      "Therefore, James writes 312 pages in a year.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "  \"James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year? Let me solve this step by step.\" #Answer: 624\n",
    ")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "out = model.generate(input_ids, max_new_tokens=150, do_sample=False, temperature=0.0)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff081cb8-51d0-4f7e-bb5f-31858f53bc5a",
   "metadata": {},
   "source": [
    "#### Gemma IT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92807d-13fd-4950-9938-680dd0a9a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
